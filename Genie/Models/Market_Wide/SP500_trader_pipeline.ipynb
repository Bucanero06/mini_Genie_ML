{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Source specific clean-up of Data\n",
    "# Only Needs to be done Once"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<logger_tt.core.LogConfig at 0x7f33be276040>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Clean Data and place into a symbols_dict\n",
    "\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vectorbtpro as vbt\n",
    "from logger_tt import setup_logging\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame  # noqa\n",
    "\n",
    "from Modules._Data_Manager import Data_Manager\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "setup_logging(full_context=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{'GE':                  High        Low       Open      Close     Volume  Adj Close\n Date                                                                        \n 1962-01-02   6.109776   5.949519   6.009615   5.989583   269568.0   1.008423\n 1962-01-03   5.959535   5.909455   5.959535   5.929487   184704.0   0.998305\n 1962-01-04   5.979567   5.809295   5.929487   5.859375   229632.0   0.986501\n 1962-01-05   5.869391   5.608974   5.859375   5.709135   340704.0   0.961206\n 1962-01-08   5.709135   5.528846   5.709135   5.699119   386880.0   0.959520\n ...               ...        ...        ...        ...        ...        ...\n 2022-10-10  65.620003  64.059998  65.050003  64.980003  3591200.0  64.980003\n 2022-10-11  65.260002  63.080002  64.489998  64.000000  5295900.0  64.000000\n 2022-10-12  65.419998  63.060001  63.689999  64.739998  5356900.0  64.739998\n 2022-10-13  68.360001  63.189999  63.689999  67.940002  7984500.0  67.940002\n 2022-10-14  68.779999  67.190002  68.300003  67.260002  3415584.0  67.260002\n \n [15303 rows x 6 columns],\n 'IBM':                   High         Low        Open       Close     Volume  \\\n Date                                                                    \n 1962-01-02    7.374124    7.291268    7.374124    7.291268   407940.0   \n 1962-01-03    7.355003    7.291268    7.291268    7.355003   305955.0   \n 1962-01-04    7.355003    7.278521    7.355003    7.281708   274575.0   \n 1962-01-05    7.272148    7.125558    7.272148    7.138305   384405.0   \n 1962-01-08    7.131931    6.947100    7.131931    7.004461   572685.0   \n ...                ...         ...         ...         ...        ...   \n 2022-10-10  119.959999  117.040001  119.790001  117.750000  5990000.0   \n 2022-10-11  119.230003  116.940002  117.459999  117.800003  4043100.0   \n 2022-10-12  118.809998  117.199997  118.000000  117.570000  3338800.0   \n 2022-10-13  122.150002  115.550003  116.099998  121.790001  5837500.0   \n 2022-10-14  122.528503  119.998199  121.800003  120.195000  1957696.0   \n \n              Adj Close  \n Date                    \n 1962-01-02    1.652505  \n 1962-01-03    1.666949  \n 1962-01-04    1.650337  \n 1962-01-05    1.617836  \n 1962-01-08    1.587502  \n ...                ...  \n 2022-10-10  117.750000  \n 2022-10-11  117.800003  \n 2022-10-12  117.570000  \n 2022-10-13  121.790001  \n 2022-10-14  120.195000  \n \n [15303 rows x 6 columns],\n 'MSFT':                   High         Low        Open       Close        Volume  \\\n Date                                                                       \n 1986-03-13    0.101563    0.088542    0.088542    0.097222  1.031789e+09   \n 1986-03-14    0.102431    0.097222    0.097222    0.100694  3.081600e+08   \n 1986-03-17    0.103299    0.100694    0.100694    0.102431  1.331712e+08   \n 1986-03-18    0.103299    0.098958    0.102431    0.099826  6.776640e+07   \n 1986-03-19    0.100694    0.097222    0.099826    0.098090  4.789440e+07   \n ...                ...         ...         ...         ...           ...   \n 2022-10-10  234.559998  226.729996  233.050003  229.250000  2.974360e+07   \n 2022-10-11  229.059998  224.110001  227.619995  225.410004  3.047400e+07   \n 2022-10-12  227.860001  223.960007  225.399994  225.750000  2.190390e+07   \n 2022-10-13  236.100006  219.130005  219.850006  234.240005  4.252370e+07   \n 2022-10-14  237.240005  229.020004  235.539993  229.119995  1.671932e+07   \n \n              Adj Close  \n Date                    \n 1986-03-13    0.060980  \n 1986-03-14    0.063158  \n 1986-03-17    0.064247  \n 1986-03-18    0.062613  \n 1986-03-19    0.061524  \n ...                ...  \n 2022-10-10  229.250000  \n 2022-10-11  225.410004  \n 2022-10-12  225.750000  \n 2022-10-13  234.240005  \n 2022-10-14  229.119995  \n \n [9224 rows x 6 columns]}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_datareader import DataReader\n",
    "\n",
    "symbols_list = ['GE', 'IBM','MSFT']\n",
    "d = {}\n",
    "for ticker in symbols_list:\n",
    "    d[ticker] = DataReader(ticker, \"yahoo\", '1950-01-01')\n",
    "\n",
    "d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 504 assets from 2022-10-03 14:23:08.302395 to 2022-10-13 14:23:08.302395 found in the S&P500\n",
      "following the added and removed tickers from the index over time\n",
      "Fetching A data\n",
      "                  High         Low        Open       Close     Volume  \\\n",
      "Date                                                                    \n",
      "2022-10-03  127.300003  121.680000  122.730003  126.379997  1343200.0   \n",
      "2022-10-04  131.559998  128.570007  128.789993  131.410004  1647800.0   \n",
      "2022-10-05  133.529999  129.500000  129.990005  132.639999  1567700.0   \n",
      "2022-10-06  133.820007  131.600006  132.300003  132.179993  1218400.0   \n",
      "2022-10-07  130.929993  126.959999  130.449997  127.440002  1154700.0   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "2022-10-03  126.379997  \n",
      "2022-10-04  131.410004  \n",
      "2022-10-05  132.639999  \n",
      "2022-10-06  132.179993  \n",
      "2022-10-07  127.440002  \n",
      "A_High\n",
      "A_Low\n",
      "A_Open\n",
      "A_Close\n",
      "A_Volume\n",
      "A_Adj Close\n",
      "Fetching AAL data\n",
      "             High    Low   Open  Close    Volume  Adj Close\n",
      "Date                                                       \n",
      "2022-10-03  12.23  11.65  12.02  11.92  41156700      11.92\n",
      "2022-10-04  13.05  12.30  12.31  12.95  47616000      12.95\n",
      "2022-10-05  12.96  12.43  12.63  12.87  28172500      12.87\n",
      "2022-10-06  13.00  12.40  12.77  12.73  31569300      12.73\n",
      "2022-10-07  12.54  11.95  12.50  12.18  37269800      12.18\n",
      "AAL_High\n",
      "AAL_Low\n",
      "AAL_Open\n",
      "AAL_Close\n",
      "AAL_Volume\n",
      "AAL_Adj Close\n",
      "Fetching AAP data\n",
      "                  High         Low        Open       Close  Volume   Adj Close\n",
      "Date                                                                          \n",
      "2022-10-03  163.979996  157.220001  157.250000  163.369995  862400  163.369995\n",
      "2022-10-04  168.130005  165.320007  165.550003  167.830002  721500  167.830002\n",
      "2022-10-05  169.600006  164.960007  166.270004  167.639999  690300  167.639999\n",
      "2022-10-06  168.610001  164.770004  167.800003  165.139999  671100  165.139999\n",
      "2022-10-07  164.029999  160.100006  163.970001  161.320007  582800  161.320007\n",
      "AAP_High\n",
      "AAP_Low\n",
      "AAP_Open\n",
      "AAP_Close\n",
      "AAP_Volume\n",
      "AAP_Adj Close\n",
      "Fetching AAPL data\n",
      "                  High         Low        Open       Close     Volume  \\\n",
      "Date                                                                    \n",
      "2022-10-03  143.070007  137.690002  138.210007  142.449997  114311700   \n",
      "2022-10-04  146.220001  144.259995  145.029999  146.100006   87830100   \n",
      "2022-10-05  147.380005  143.009995  144.070007  146.399994   79471000   \n",
      "2022-10-06  147.539993  145.220001  145.809998  145.429993   68402200   \n",
      "2022-10-07  143.100006  139.449997  142.539993  140.089996   85859100   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "2022-10-03  142.449997  \n",
      "2022-10-04  146.100006  \n",
      "2022-10-05  146.399994  \n",
      "2022-10-06  145.429993  \n",
      "2022-10-07  140.089996  \n",
      "AAPL_High\n",
      "AAPL_Low\n",
      "AAPL_Open\n",
      "AAPL_Close\n",
      "AAPL_Volume\n",
      "AAPL_Adj Close\n",
      "Fetching ABBV data\n",
      "                  High         Low        Open       Close     Volume  \\\n",
      "Date                                                                    \n",
      "2022-10-03  138.800003  135.309998  135.880005  138.320007  7437500.0   \n",
      "2022-10-04  142.720001  137.759995  139.210007  141.990005  6612300.0   \n",
      "2022-10-05  144.470001  141.110001  141.470001  143.330002  4777900.0   \n",
      "2022-10-06  142.990005  139.899994  142.789993  140.289993  5124800.0   \n",
      "2022-10-07  140.050003  137.550003  139.009995  138.759995  4648100.0   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "2022-10-03  136.926727  \n",
      "2022-10-04  140.559753  \n",
      "2022-10-05  141.886261  \n",
      "2022-10-06  138.876862  \n",
      "2022-10-07  137.362274  \n",
      "ABBV_High\n",
      "ABBV_Low\n",
      "ABBV_Open\n",
      "ABBV_Close\n",
      "ABBV_Volume\n",
      "ABBV_Adj Close\n",
      "Fetching ABC data\n",
      "                  High         Low        Open       Close   Volume  \\\n",
      "Date                                                                  \n",
      "2022-10-03  139.750000  135.809998  136.270004  139.309998  1016400   \n",
      "2022-10-04  142.850006  139.570007  140.020004  142.729996  1074400   \n",
      "2022-10-05  143.520004  141.100006  141.839996  142.449997   728000   \n",
      "2022-10-06  142.380005  139.619995  142.139999  139.910004   938800   \n",
      "2022-10-07  139.270004  136.869995  138.800003  137.350006  1034600   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "2022-10-03  139.309998  \n",
      "2022-10-04  142.729996  \n",
      "2022-10-05  142.449997  \n",
      "2022-10-06  139.910004  \n",
      "2022-10-07  137.350006  \n",
      "ABC_High\n",
      "ABC_Low\n",
      "ABC_Open\n",
      "ABC_Close\n",
      "ABC_Volume\n",
      "ABC_Adj Close\n",
      "Fetching ABMD data\n",
      "                  High         Low        Open       Close  Volume   Adj Close\n",
      "Date                                                                          \n",
      "2022-10-03  255.830002  244.990005  248.710007  255.149994  485500  255.149994\n",
      "2022-10-04  264.630005  260.619995  261.489990  264.010010  250200  264.010010\n",
      "2022-10-05  265.399994  258.799988  260.790009  264.260010  249900  264.260010\n",
      "2022-10-06  270.750000  261.709991  264.149994  267.209991  359500  267.209991\n",
      "2022-10-07  266.529999  255.479996  263.369995  257.089996  243000  257.089996\n",
      "ABMD_High\n",
      "ABMD_Low\n",
      "ABMD_Open\n",
      "ABMD_Close\n",
      "ABMD_Volume\n",
      "ABMD_Adj Close\n",
      "Fetching ABT data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Models.Market_Wide.SP500_Predictor import fetch_index_asset_prices, fetch_alternative_data\n",
    "\n",
    "\"\"\"Load Raw Data\"\"\"\n",
    "\n",
    "# Duka allows for download multiple assets at once however due to the limit imposed by the data source it tends\n",
    "# to cause perfoimary issues\n",
    "# ! duka EURUSD  -s 2020-09-29  --header && duka USDJPY -s 2020-09-29  --header &&\n",
    "#       duka GBPUSD  -s 2020-09-29  --header && duka AUDUSD  -s 2020-09-29  --header &&\n",
    "#       duka USDCAD -s 2020-09-29  --header && duka USDCNY  -s 2020-09-29  --header &&\n",
    "#       duka USDCHF  -s 2020-09-29  --header && duka EURGBP  -s 2020-09-29  --header &&\n",
    "#       duka USDKRW  -s 2020-09-29  --header\n",
    "\n",
    "# file_names = [path.split(\"/\")[-1] for path in\n",
    "#               glob.glob(\"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/*.csv\")]\n",
    "#\n",
    "# asset_data_obj = Data_Manager().fetch_data(data_file_names=file_names,\n",
    "#                                            data_file_dirs=[\n",
    "#                                                \"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/\"],\n",
    "#                                            )\n",
    "asset_data_obj = fetch_index_asset_prices(days_to_fetch=10, last_day=datetime.today() - timedelta(days=1)\n",
    "                                          , index_name=\"SP500\", read_from_file=False)\n",
    "alternative_data_dir = \"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Alternative_Data\"\n",
    "\n",
    "# Label data\n",
    "AD_df_obj = fetch_alternative_data(alternative_data_dir)\n",
    "\n",
    "# Print all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "asset_data_obj.save(\"sp500_data_obj_data\")\n",
    "AD_df_obj.save(\"AD_df_obj_data\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "asset_data_obj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AD_df_obj\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"STEP __N__\"\"\"\n",
    "\"\"\"Load Combined Data Pickle\"\"\"\n",
    "symbols_dict_obj = vbt.Data.load(\"forex_tick_data\").data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Genie as genie\n",
    "\n",
    "_preprocessed_data_dict = dict()\n",
    "\n",
    "for asset in symbols_dict_obj.keys():\n",
    "    # smalled_index = symbols_dict_obj[asset].first_valid_index()\n",
    "    # largest_index = symbols_dict_obj[asset].last_valid_index()\n",
    "    #\n",
    "    # _preprocessed_data_dict[asset[:6]] = symbols_dict_obj[asset][smalled_index:largest_index].copy()\n",
    "    _preprocessed_data_dict[asset[:6]] = symbols_dict_obj[asset].copy()\n",
    "    asset = asset[:6]\n",
    "\n",
    "    _preprocessed_data_dict[asset] = _preprocessed_data_dict[asset].dropna()\n",
    "    # spread = abs(\n",
    "    #     preprocessed_data_dict[asset][\"ask\"] - preprocessed_data_dict[asset][\"bid\"])\n",
    "\n",
    "    threshold =\n",
    "    (_preprocessed_data_dict[asset]['ask'] * _preprocessed_data_dict[asset]['ask_volume']).vbt.resample_apply('1 min',\n",
    "                                                                                                              vbt.nb.min_reduce_nb).dropna().describe()[\n",
    "        '75%'].mean()\n",
    "    print(f'{threshold = }')\n",
    "    _preprocessed_data_dict[asset] = genie.ml.data_structures.get_dollar_bars(\n",
    "        _preprocessed_data_dict[asset].reset_index()[[\"time\", \"ask\", \"ask_volume\"]],\n",
    "        threshold=threshold,\n",
    "        batch_size=1000000, verbose=True)\n",
    "\n",
    "    # Constant Run Bars\n",
    "    # _preprocessed_data_dict[asset] = genie.ml.data_structures.get_const_dollar_run_bars(\n",
    "    #     _preprocessed_data_dict[asset].reset_index()[[\"time\", \"ask\", \"ask_volume\"]], num_prev_bars=3,\n",
    "    #     exp_num_ticks_init=100,\n",
    "    #     expected_imbalance_window=100)[0].set_index(\"date_time\")\n",
    "\n",
    "    _preprocessed_data_dict[asset].to_pickle(\n",
    "        f'/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/{asset}_dollar_run_bars.pickle')\n",
    "\n",
    "    # print(preprocessed_data_dict[asset])\n",
    "    print(_preprocessed_data_dict[asset])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_preprocessed_data_dict.keys()\n",
    "preprocessed_data_dict = _preprocessed_data_dict\n",
    "preprocessed_data_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/*_dollar_run_bars.pickle\")\n",
    "preprocessed_data_dict = dict()\n",
    "for path in file_paths:\n",
    "    preprocessed_data_dict[path.split(\"/\")[-1][:6]] = pd.read_pickle(path).set_index('date_time')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Step 1 - A"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Compute y'''\n",
    "from multiprocessing import cpu_count\n",
    "from __utils import genie_strategy_wrapper\n",
    "import numpy as np\n",
    "\n",
    "parametrized_genie_strategy_wrapper = vbt.parameterized(genie_strategy_wrapper,\n",
    "                                                        # merge_func=\"concat\",\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        chunk_len='auto',\n",
    "                                                        engine='ray',\n",
    "                                                        show_progress=True,\n",
    "                                                        init_kwargs={\n",
    "                                                            # 'address': 'auto',\n",
    "                                                            'num_cpus': cpu_count() - 2,\n",
    "                                                            # 'n_chunks':\"auto\",\n",
    "                                                            # 'memory': 100 * 10 ** 9,\n",
    "                                                            # 'object_store_memory': 100 * 10 ** 9,\n",
    "                                                        },\n",
    "                                                        )\n",
    "\n",
    "assets_data_list = [preprocessed_data_dict[asset_ohlc] for asset_ohlc in preprocessed_data_dict.keys()]\n",
    "result = parametrized_genie_strategy_wrapper(\n",
    "    asset_ohlc=vbt.Param(\n",
    "        assets_data_list\n",
    "        # , name='symbols'\n",
    "    ),\n",
    "    threads_per_worker=np.floor((cpu_count() - 2) / len(assets_data_list)).astype(int),\n",
    "\n",
    ")\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "first_index = preprocessed_data_dict[list(preprocessed_data_dict.keys())[0]].index[0]\n",
    "last_index = preprocessed_data_dict[list(preprocessed_data_dict.keys())[0]].index[-1]\n",
    "\n",
    "for index, asset_ohlc in enumerate(preprocessed_data_dict.keys()):\n",
    "    # print(result[index])\n",
    "    preprocessed_data_dict[asset_ohlc][\"target\"] = result[index]\n",
    "    preprocessed_data_dict[asset_ohlc] = preprocessed_data_dict[asset_ohlc].fillna(\n",
    "        0)  #.fillna(method=\"ffill\")  #.dropna(0)\n",
    "    # first_index = preprocessed_data_dict[asset_ohlc].index[0] if first_index < preprocessed_data_dict[asset_ohlc].index[\n",
    "    #     0] else first_index\n",
    "    # last_index = preprocessed_data_dict[asset_ohlc].index[-1] if last_index > preprocessed_data_dict[asset_ohlc].index[\n",
    "    #     -1] else last_index\n",
    "\n",
    "    # print(f\"{(preped_forex_data[asset_ohlc]['target'] == 1).sum() = }\")\n",
    "    # print(f\"{(preped_forex_data[asset_ohlc]['target'] == -1).sum() = }\")\n",
    "    # print(f\"{(preped_forex_data[asset_ohlc]['target'] == 0).sum() = }\")\n",
    "\n",
    "    print(preprocessed_data_dict[asset_ohlc])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessed_data_dict[asset_ohlc]['target'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_prefix_to_df_columns(df, prefix):\n",
    "    df.columns = [prefix + \"_\" + col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "for asset_name in preprocessed_data_dict.keys():\n",
    "    # preprocessed_data_dict[asset_name] = preprocessed_data_dict[asset_name].loc[first_index:last_index]\n",
    "    preprocessed_data_dict[asset_name] = add_prefix_to_df_columns(preprocessed_data_dict[asset_name], asset_name)\n",
    "\n",
    "    print(preprocessed_data_dict[asset_name].head())\n",
    "preprocessed_data_dict\n",
    "# e.g."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat(preprocessed_data_dict, axis=1).reset_index()\n",
    "\n",
    "cols = list(df.columns.droplevel(0))\n",
    "cols[0] = 'date_time'\n",
    "df.columns = cols\n",
    "df = df.ffill()\n",
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.set_index(\"date_time\")\n",
    "df = df.tz_convert(None)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('mmt_triplebarrier_h2o_test_dataset.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forex_data = pd.read_csv('mmt_triplebarrier_h2o_test_dataset.csv')\n",
    "forex_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forex_data = df.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h2o.init(nthreads=4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Identify predictors and response\n",
    "x = list(forex_data.columns)\n",
    "y = \"EURUSD_target\"\n",
    "x.remove(y)\n",
    "\n",
    "# pandas dataframe to h2o dataframe\n",
    "h2o_forex_data = h2o.H2OFrame(forex_data)\n",
    "\n",
    "# Split data into train and test sets,and  validation\n",
    "train, valid, test = h2o_forex_data.split_frame([0.8, 0.1], seed=1234)\n",
    "\n",
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "valid[y] = valid[y].asfactor()\n",
    "test[y] = test[y].asfactor()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run AutoML for 20 base models\n",
    "aml = H2OAutoML(max_models=20, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 - B\n",
    "Compute Alphas on Prices and add them to their respective dataframes in the symbols dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Compute alphas\"\"\"\n",
    "# wqa_indicator = vbt.wqa101(53)\n",
    "# value = wqa_indicator.run(open=asset_ohlc['open'], high=asset_ohlc['high'], low=asset_ohlc['low'],\n",
    "#                           close=asset_ohlc['close'], volume=asset_ohlc['volume'])\n",
    "# value.out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2\n",
    "Transform symbols_dict into a flat_dataframe for tsfresh use and a performace boost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Prep for TSFRESH\"\"\"\n",
    "# forex_data_dict = preprocessed_data_dict.copy()\n",
    "# forex_concated = pd.concat(forex_data_dict, axis=0)  # [\"symbols\"].extend(list(forex_data_dict.keys()))\n",
    "# pd.melt(forex_concated)\n",
    "# forex_concated[\"id\"] = forex_concated.index\n",
    "forex_concated = forex_data.reset_index()\n",
    "# Change level_0 column to symbol\n",
    "forex_concated = forex_concated.rename(columns={'index': 'id'})\n",
    "\n",
    "# forex_concated = forex_concated.drop(columns=[\"level_0\"])\n",
    "forex_concated.to_pickle('flat_forex_dataframe.pickle')\n",
    "print(forex_concated)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3\n",
    "1. Create a rolling time series (n windows)\n",
    "2. Compute the features (In this case only those that can be efficiently computed) \"X\".\n",
    "3. Prepare y\n",
    "4. Select only those features based on their p-value significance in the classification or regression target (y)\n",
    "5."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"TSFRESH Processes\"\"\"\n",
    "import vectorbtpro as vbt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tsfresh_ready_data = pd.read_pickle(\"flat_forex_dataframe.pickle\")\n",
    "tsfresh_ready_data = tsfresh_ready_data.dropna()\n",
    "\n",
    "# %%\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "\n",
    "_tsfresh_ready_data_rolled = roll_time_series(tsfresh_ready_data,\n",
    "                                              column_id=\"id\",\n",
    "                                              column_sort=\"date_time\",\n",
    "                                              max_timeshift=1, min_timeshift=1, rolling_direction=1)\n",
    "# %%\n",
    "_tsfresh_ready_data_rolled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_tsfresh_ready_data_rolled.to_pickle('tsfresh_ready_data_rolled.pickle')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tsfresh_ready_data_rolled = pd.read_pickle(\"tsfresh_ready_data_rolled.pickle\")\n",
    "# tsfresh_ready_data_rolled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = tsfresh_ready_data_rolled.pop('target')\n",
    "target = target.astype(int)\n",
    "target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"\"\"TSFRESH Processes\"\"\"\n",
    "# import vectorbtpro as vbt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tsfresh.feature_extraction import extract_features\n",
    "#\n",
    "# tsfresh_ready_data = pd.read_pickle(\"flat_forex_dataframe.pickle\")\n",
    "# tsfresh_ready_data = tsfresh_ready_data.dropna()\n",
    "#\n",
    "# # %%\n",
    "# from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "#\n",
    "# tsfresh_ready_data_rolled = roll_time_series(tsfresh_ready_data, column_id=\"id\",\n",
    "#                                              # column_sort=\"date_time\",\n",
    "#                                              max_timeshift=200, min_timeshift=200, rolling_direction=1)\n",
    "# # %%\n",
    "\n",
    "# %%\n",
    "from tsfresh.utilities.distribution import ClusterDaskDistributor, LocalDaskDistributor  # noqa: F401\n",
    "from tsfresh.utilities.distribution import MultiprocessingDistributor  # noqa: F401\n",
    "from tsfresh.feature_extraction.settings import ComprehensiveFCParameters, EfficientFCParameters  # noqa: F401\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "\n",
    "# settings = EfficientFCParameters()\n",
    "settings = {\n",
    "    # \"length\": None,\n",
    "    \"large_standard_deviation\": [{\"r\": 0.05}, {\"r\": 0.1}]\n",
    "}\n",
    "\n",
    "# We construct a Distributor that will spawn the calculations\n",
    "# over four threads on the local machine\n",
    "# Distributor = MultiprocessingDistributor(n_workers=14,\n",
    "#                                          disable_progressbar=False,\n",
    "#                                          progressbar_title=\"Feature Extraction\")\n",
    "\n",
    "# Distributor = ClusterDaskDistributor(address='localhost:8786')\n",
    "\n",
    "\n",
    "# Distributor = LocalDaskDistributor(n_workers=20)\n",
    "# X_tsfresh contains the extracted tsfresh features\n",
    "# just to pass the Distributor object to\n",
    "# the feature extraction, along with the other parameters\n",
    "\n",
    "\n",
    "X_tsfresh = extract_features(\n",
    "    timeseries_container=tsfresh_ready_data_rolled,\n",
    "    column_id='id',\n",
    "    column_sort='date_time',\n",
    "    # distributor=Distributor,\n",
    "    n_jobs=16,\n",
    "    default_fc_parameters=settings,\n",
    ")\n",
    "# %%\n",
    "X_tsfresh\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_tsfresh.to_pickle('X_tsfresh.pickle')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tsfresh import select_features\n",
    "\n",
    "# which are now filtered to only contain relevant features\n",
    "X_tsfresh_filtered = select_features(X_tsfresh, target)\n",
    "X_tsfresh_filtered\n",
    "# # # we can easily construct the corresponding settings object\n",
    "# kind_to_fc_parameters = feature_extraction.settings.from_columns(X_tsfresh_filtered)\n",
    "# %%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from Modules.Utils import mltidx_df_to_dict\n",
    "#\n",
    "# preped_forex_data_dict = mltidx_df_to_dict(preped_forex_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # which are now filtered to only contain relevant features\n",
    "# X_tsfresh_filtered = tsf.some_feature_selection(X_tsfresh, y, ....)\n",
    "#\n",
    "# # we can easily construct the corresponding settings object\n",
    "# kind_to_fc_parameters = tsf.feature_extraction.settings.from_columns(X_tsfresh_filtered)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from __utils import genie_strategy_wrapper\n",
    "\n",
    "\"\"\"Pass Through a Strategy\"\"\"\n",
    "clean_forex_data_dict = dict()\n",
    "for asset_ohlc in tuple(forex_data_dict.data.keys()):\n",
    "    clean_forex_data_dict[asset_ohlc] = forex_data_dict[asset_ohlc].set_index(\"datetime\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "parametrized_genie_strategy_wrapper = vbt.parameterized(genie_strategy_wrapper,\n",
    "                                                        # merge_func=\"concat\",\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        chunk_len='auto',\n",
    "                                                        engine='ray',\n",
    "                                                        show_progress=True,\n",
    "                                                        init_kwargs={\n",
    "                                                            # 'address': 'auto',\n",
    "                                                            'num_cpus': cpu_count() - 2,\n",
    "                                                            # 'n_chunks':\"auto\",\n",
    "                                                            # 'memory': 100 * 10 ** 9,\n",
    "                                                            # 'object_store_memory': 100 * 10 ** 9,\n",
    "                                                        },\n",
    "                                                        )\n",
    "\n",
    "assets_list = [clean_forex_data_dict[asset_ohlc] for asset_ohlc in clean_forex_data_dict.keys()]\n",
    "result = parametrized_genie_strategy_wrapper(\n",
    "    asset_ohlc=vbt.Param(\n",
    "        assets_list\n",
    "        # , name='symbols'\n",
    "    ),\n",
    "    threads_per_worker=np.floor((cpu_count() - 2) / len(assets_list)).astype(int),\n",
    "\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Alternative Data ... \"\"\"\n",
    "\n",
    "AD_dobj = Data_Manager().fetch_data(\n",
    "    data_file_names=\"AD_df.csv\",\n",
    "    data_file_dirs=\n",
    "    [\"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data\", \".\",\n",
    "     \"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Alternative_Data\"],\n",
    ")\n",
    "AD_dobj.data"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
