{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Source specific clean-up of Data\n",
    "# Only Needs to be done Once"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Clean Data and place into a symbols_dict\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import vectorbtpro as vbt\n",
    "from logger_tt import setup_logging\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame  # noqa\n",
    "\n",
    "from Modules._Data_Manager import Data_Manager\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "setup_logging(full_context=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Load Raw Data\"\"\"\n",
    "\n",
    "# Duka allows for download multiple assets at once however due to the limit imposed by the data source it tends\n",
    "# to cause perfoimary issues\n",
    "# ! duka EURUSD  -s 2020-09-29  --header && duka USDJPY -s 2020-09-29  --header &&\n",
    "#       duka GBPUSD  -s 2020-09-29  --header && duka AUDUSD  -s 2020-09-29  --header &&\n",
    "#       duka USDCAD -s 2020-09-29  --header && duka USDCNY  -s 2020-09-29  --header &&\n",
    "#       duka USDCHF  -s 2020-09-29  --header && duka EURGBP  -s 2020-09-29  --header &&\n",
    "#       duka USDKRW  -s 2020-09-29  --header\n",
    "\n",
    "file_names = [path.split(\"/\")[-1] for path in\n",
    "              glob.glob(\"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/*.csv\")]\n",
    "\n",
    "asset_data_obj = Data_Manager().fetch_data(data_file_names=file_names,\n",
    "                                           data_file_dirs=[\n",
    "                                               \"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/\"],\n",
    "                                           )\n",
    "\n",
    "# Print all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "asset_data_obj.save(\"forex_tick_data\")\n",
    "\n",
    "asset_data_obj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"STEP __N__\"\"\"\n",
    "\"\"\"Load Combined Data Pickle\"\"\"\n",
    "symbols_dict_obj = vbt.Data.load(\"forex_tick_data\").data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Genie as genie\n",
    "\n",
    "_preprocessed_data_dict = dict()\n",
    "\n",
    "for asset in symbols_dict_obj.keys():\n",
    "    # smalled_index = symbols_dict_obj[asset].first_valid_index()\n",
    "    # largest_index = symbols_dict_obj[asset].last_valid_index()\n",
    "    #\n",
    "    # _preprocessed_data_dict[asset[:6]] = symbols_dict_obj[asset][smalled_index:largest_index].copy()\n",
    "    _preprocessed_data_dict[asset[:6]] = symbols_dict_obj[asset].copy()\n",
    "    asset = asset[:6]\n",
    "\n",
    "    _preprocessed_data_dict[asset] = _preprocessed_data_dict[asset].dropna()\n",
    "    # spread = abs(\n",
    "    #     preprocessed_data_dict[asset][\"ask\"] - preprocessed_data_dict[asset][\"bid\"])\n",
    "\n",
    "    threshold = (_preprocessed_data_dict[asset]['ask'] * _preprocessed_data_dict[asset]['ask_volume']).vbt.resample_apply('1 min',\n",
    "                                                                                                              vbt.nb.min_reduce_nb).dropna().describe()['75%'].mean()\n",
    "    print(f'{threshold = }')\n",
    "    _preprocessed_data_dict[asset] = genie.ml.data_structures.get_dollar_bars(\n",
    "        _preprocessed_data_dict[asset].reset_index()[[\"time\", \"ask\", \"ask_volume\"]],\n",
    "        threshold=threshold,\n",
    "        batch_size=1000000, verbose=True)\n",
    "\n",
    "    # Constant Run Bars\n",
    "    # _preprocessed_data_dict[asset] = genie.ml.data_structures.get_const_dollar_run_bars(\n",
    "    #     _preprocessed_data_dict[asset].reset_index()[[\"time\", \"ask\", \"ask_volume\"]], num_prev_bars=3,\n",
    "    #     exp_num_ticks_init=100,\n",
    "    #     expected_imbalance_window=100)[0].set_index(\"date_time\")\n",
    "\n",
    "    _preprocessed_data_dict[asset].to_pickle(\n",
    "        f'/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/{asset}_dollar_run_bars.pickle')\n",
    "\n",
    "    # print(preprocessed_data_dict[asset])\n",
    "    print(_preprocessed_data_dict[asset])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_preprocessed_data_dict.keys()\n",
    "preprocessed_data_dict = _preprocessed_data_dict\n",
    "preprocessed_data_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data/*_dollar_run_bars.pickle\")\n",
    "preprocessed_data_dict = dict()\n",
    "for path in file_paths:\n",
    "    preprocessed_data_dict[path.split(\"/\")[-1][:6]] = pd.read_pickle(path).set_index('date_time')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Step 1 - A"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Compute y'''\n",
    "from multiprocessing import cpu_count\n",
    "from __utils import genie_strategy_wrapper\n",
    "import numpy as np\n",
    "\n",
    "parametrized_genie_strategy_wrapper = vbt.parameterized(genie_strategy_wrapper,\n",
    "                                                        # merge_func=\"concat\",\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        chunk_len='auto',\n",
    "                                                        engine='ray',\n",
    "                                                        show_progress=True,\n",
    "                                                        init_kwargs={\n",
    "                                                            # 'address': 'auto',\n",
    "                                                            'num_cpus': cpu_count() - 2,\n",
    "                                                            # 'n_chunks':\"auto\",\n",
    "                                                            # 'memory': 100 * 10 ** 9,\n",
    "                                                            # 'object_store_memory': 100 * 10 ** 9,\n",
    "                                                        },\n",
    "                                                        )\n",
    "\n",
    "assets_data_list = [preprocessed_data_dict[asset_ohlc] for asset_ohlc in preprocessed_data_dict.keys()]\n",
    "result = parametrized_0_strategy_wrapper(\n",
    "    asset_ohlc=vbt.Param(\n",
    "        assets_data_list\n",
    "        # , name='symbols'\n",
    "    ),\n",
    "    threads_per_worker=np.floor((cpu_count() - 2) / len(assets_data_list)).astype(int),\n",
    "\n",
    ")\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "first_index = preprocessed_data_dict[list(preprocessed_data_dict.keys())[0]].index[0]\n",
    "last_index = preprocessed_data_dict[list(preprocessed_data_dict.keys())[0]].index[-1]\n",
    "\n",
    "for index, asset_ohlc in enumerate(preprocessed_data_dict.keys()):\n",
    "    # print(result[index])\n",
    "    preprocessed_data_dict[asset_ohlc][\"target\"] = result[index]\n",
    "    preprocessed_data_dict[asset_ohlc] = preprocessed_data_dict[asset_ohlc].fillna(0)#.fillna(method=\"ffill\")  #.dropna(0)\n",
    "    # first_index = preprocessed_data_dict[asset_ohlc].index[0] if first_index < preprocessed_data_dict[asset_ohlc].index[\n",
    "    #     0] else first_index\n",
    "    # last_index = preprocessed_data_dict[asset_ohlc].index[-1] if last_index > preprocessed_data_dict[asset_ohlc].index[\n",
    "    #     -1] else last_index\n",
    "\n",
    "    # print(f\"{(preped_forex_data[asset_ohlc]['target'] == 1).sum() = }\")\n",
    "    # print(f\"{(preped_forex_data[asset_ohlc]['target'] == -1).sum() = }\")\n",
    "    # print(f\"{(preped_forex_data[asset_ohlc]['target'] == 0).sum() = }\")\n",
    "\n",
    "    print(preprocessed_data_dict[asset_ohlc])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessed_data_dict[asset_ohlc]['target'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_prefix_to_df_columns(df, prefix):\n",
    "    df.columns = [prefix + \"_\" + col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "for asset_name in preprocessed_data_dict.keys():\n",
    "    # preprocessed_data_dict[asset_name] = preprocessed_data_dict[asset_name].loc[first_index:last_index]\n",
    "    preprocessed_data_dict[asset_name] = add_prefix_to_df_columns(preprocessed_data_dict[asset_name], asset_name)\n",
    "\n",
    "    print(preprocessed_data_dict[asset_name].head())\n",
    "preprocessed_data_dict\n",
    "# e.g."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat(preprocessed_data_dict, axis=1).reset_index()\n",
    "\n",
    "cols = list(df.columns.droplevel(0))\n",
    "cols[0] = 'date_time'\n",
    "df.columns = cols\n",
    "df = df.ffill()\n",
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df=df.set_index(\"date_time\")\n",
    "df = df.tz_convert(None)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('mmt_triplebarrier_h2o_test_dataset.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forex_data = pd.read_csv('mmt_triplebarrier_h2o_test_dataset.csv')\n",
    "forex_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forex_data = df.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h2o.init(nthreads=4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Identify predictors and response\n",
    "x = list(forex_data.columns)\n",
    "y = \"EURUSD_target\"\n",
    "x.remove(y)\n",
    "\n",
    "# pandas dataframe to h2o dataframe\n",
    "h2o_forex_data = h2o.H2OFrame(forex_data)\n",
    "\n",
    "# Split data into train and test sets,and  validation\n",
    "train, valid, test = h2o_forex_data.split_frame([0.8, 0.1], seed=1234)\n",
    "\n",
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "valid[y] = valid[y].asfactor()\n",
    "test[y] = test[y].asfactor()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run AutoML for 20 base models\n",
    "aml = H2OAutoML(max_models=20, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 - B\n",
    "Compute Alphas on Prices and add them to their respective dataframes in the symbols dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Compute alphas\"\"\"\n",
    "# wqa_indicator = vbt.wqa101(53)\n",
    "# value = wqa_indicator.run(open=asset_ohlc['open'], high=asset_ohlc['high'], low=asset_ohlc['low'],\n",
    "#                           close=asset_ohlc['close'], volume=asset_ohlc['volume'])\n",
    "# value.out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2\n",
    "Transform symbols_dict into a flat_dataframe for tsfresh use and a performace boost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Prep for TSFRESH\"\"\"\n",
    "# forex_data_dict = preprocessed_data_dict.copy()\n",
    "# forex_concated = pd.concat(forex_data_dict, axis=0)  # [\"symbols\"].extend(list(forex_data_dict.keys()))\n",
    "# pd.melt(forex_concated)\n",
    "# forex_concated[\"id\"] = forex_concated.index\n",
    "forex_concated = forex_data.reset_index()\n",
    "# Change level_0 column to symbol\n",
    "forex_concated = forex_concated.rename(columns={'index': 'id'})\n",
    "\n",
    "# forex_concated = forex_concated.drop(columns=[\"level_0\"])\n",
    "forex_concated.to_pickle('flat_forex_dataframe.pickle')\n",
    "print(forex_concated)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3\n",
    "1. Create a rolling time series (n windows)\n",
    "2. Compute the features (In this case only those that can be efficiently computed) \"X\".\n",
    "3. Prepare y\n",
    "4. Select only those features based on their p-value significance in the classification or regression target (y)\n",
    "5."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"TSFRESH Processes\"\"\"\n",
    "import vectorbtpro as vbt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tsfresh_ready_data = pd.read_pickle(\"flat_forex_dataframe.pickle\")\n",
    "tsfresh_ready_data = tsfresh_ready_data.dropna()\n",
    "\n",
    "# %%\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "\n",
    "_tsfresh_ready_data_rolled = roll_time_series(tsfresh_ready_data,\n",
    "                                              column_id=\"id\",\n",
    "                                              column_sort=\"date_time\",\n",
    "                                              max_timeshift=1, min_timeshift=1, rolling_direction=1)\n",
    "# %%\n",
    "_tsfresh_ready_data_rolled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_tsfresh_ready_data_rolled.to_pickle('tsfresh_ready_data_rolled.pickle')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tsfresh_ready_data_rolled = pd.read_pickle(\"tsfresh_ready_data_rolled.pickle\")\n",
    "# tsfresh_ready_data_rolled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = tsfresh_ready_data_rolled.pop('target')\n",
    "target = target.astype(int)\n",
    "target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"\"\"TSFRESH Processes\"\"\"\n",
    "# import vectorbtpro as vbt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tsfresh.feature_extraction import extract_features\n",
    "#\n",
    "# tsfresh_ready_data = pd.read_pickle(\"flat_forex_dataframe.pickle\")\n",
    "# tsfresh_ready_data = tsfresh_ready_data.dropna()\n",
    "#\n",
    "# # %%\n",
    "# from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "#\n",
    "# tsfresh_ready_data_rolled = roll_time_series(tsfresh_ready_data, column_id=\"id\",\n",
    "#                                              # column_sort=\"date_time\",\n",
    "#                                              max_timeshift=200, min_timeshift=200, rolling_direction=1)\n",
    "# # %%\n",
    "\n",
    "# %%\n",
    "from tsfresh.utilities.distribution import ClusterDaskDistributor, LocalDaskDistributor  # noqa: F401\n",
    "from tsfresh.utilities.distribution import MultiprocessingDistributor  # noqa: F401\n",
    "from tsfresh.feature_extraction.settings import ComprehensiveFCParameters, EfficientFCParameters  # noqa: F401\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "\n",
    "# settings = EfficientFCParameters()\n",
    "settings = {\n",
    "    # \"length\": None,\n",
    "    \"large_standard_deviation\": [{\"r\": 0.05}, {\"r\": 0.1}]\n",
    "}\n",
    "\n",
    "# We construct a Distributor that will spawn the calculations\n",
    "# over four threads on the local machine\n",
    "# Distributor = MultiprocessingDistributor(n_workers=14,\n",
    "#                                          disable_progressbar=False,\n",
    "#                                          progressbar_title=\"Feature Extraction\")\n",
    "\n",
    "# Distributor = ClusterDaskDistributor(address='localhost:8786')\n",
    "\n",
    "\n",
    "# Distributor = LocalDaskDistributor(n_workers=20)\n",
    "# X_tsfresh contains the extracted tsfresh features\n",
    "# just to pass the Distributor object to\n",
    "# the feature extraction, along with the other parameters\n",
    "\n",
    "\n",
    "X_tsfresh = extract_features(\n",
    "    timeseries_container=tsfresh_ready_data_rolled,\n",
    "    column_id='id',\n",
    "    column_sort='date_time',\n",
    "    # distributor=Distributor,\n",
    "    n_jobs=16,\n",
    "    default_fc_parameters=settings,\n",
    ")\n",
    "# %%\n",
    "X_tsfresh\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_tsfresh.to_pickle('X_tsfresh.pickle')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tsfresh import select_features\n",
    "\n",
    "# which are now filtered to only contain relevant features\n",
    "X_tsfresh_filtered = select_features(X_tsfresh, target)\n",
    "X_tsfresh_filtered\n",
    "# # # we can easily construct the corresponding settings object\n",
    "# kind_to_fc_parameters = feature_extraction.settings.from_columns(X_tsfresh_filtered)\n",
    "# %%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from Modules.Utils import mltidx_df_to_dict\n",
    "#\n",
    "# preped_forex_data_dict = mltidx_df_to_dict(preped_forex_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # which are now filtered to only contain relevant features\n",
    "# X_tsfresh_filtered = tsf.some_feature_selection(X_tsfresh, y, ....)\n",
    "#\n",
    "# # we can easily construct the corresponding settings object\n",
    "# kind_to_fc_parameters = tsf.feature_extraction.settings.from_columns(X_tsfresh_filtered)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from __utils import genie_strategy_wrapper\n",
    "\n",
    "\"\"\"Pass Through a Strategy\"\"\"\n",
    "clean_forex_data_dict = dict()\n",
    "for asset_ohlc in tuple(forex_data_dict.data.keys()):\n",
    "    clean_forex_data_dict[asset_ohlc] = forex_data_dict[asset_ohlc].set_index(\"datetime\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "parametrized_genie_strategy_wrapper = vbt.parameterized(genie_strategy_wrapper,\n",
    "                                                        # merge_func=\"concat\",\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        # n_chunks=np.floor(param_combinations.shape[0]/4).astype(int),\n",
    "                                                        chunk_len='auto',\n",
    "                                                        engine='ray',\n",
    "                                                        show_progress=True,\n",
    "                                                        init_kwargs={\n",
    "                                                            # 'address': 'auto',\n",
    "                                                            'num_cpus': cpu_count() - 2,\n",
    "                                                            # 'n_chunks':\"auto\",\n",
    "                                                            # 'memory': 100 * 10 ** 9,\n",
    "                                                            # 'object_store_memory': 100 * 10 ** 9,\n",
    "                                                        },\n",
    "                                                        )\n",
    "\n",
    "assets_list = [clean_forex_data_dict[asset_ohlc] for asset_ohlc in clean_forex_data_dict.keys()]\n",
    "result = parametrized_genie_strategy_wrapper(\n",
    "    asset_ohlc=vbt.Param(\n",
    "        assets_list\n",
    "        # , name='symbols'\n",
    "    ),\n",
    "    threads_per_worker=np.floor((cpu_count() - 2) / len(assets_list)).astype(int),\n",
    "\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Alternative Data ... \"\"\"\n",
    "\n",
    "AD_dobj = Data_Manager().fetch_data(\n",
    "    data_file_names=\"AD_df.csv\",\n",
    "    data_file_dirs=\n",
    "    [\"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Forex_Tick_Data\", \".\",\n",
    "     \"/home/ruben/PycharmProjects/mini_Genie_ML/Datas/Alternative_Data\"],\n",
    ")\n",
    "AD_dobj.data"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
