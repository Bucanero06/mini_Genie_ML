{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas_datareader import data as datareader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import math\n",
    "import random\n",
    "from itertools import combinations\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import r2_score, matthews_corrcoef, f1_score, mean_absolute_error\n",
    "from sklearn.model_selection import BaseCrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlfinlab.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmlfinlab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmlfinlab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmicrostructural_features\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfirst_generation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_roll_measure, get_roll_impact,get_corwin_schultz_estimator,get_bekker_parkinson_vol\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmlfinlab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmlfinlab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmicrostructural_features\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msecond_generation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_bar_based_kyle_lambda, get_bar_based_amihud_lambda, get_bar_based_hasbrouck_lambda\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mta\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m add_all_ta_features\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject3/mlfinlab/mlfinlab/__init__.py:7\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwebbrowser\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtextwrap\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlfinlab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_validation\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcross_validation\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlfinlab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_structures\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdata_structures\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlfinlab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmulti_product\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmulti_product\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'mlfinlab.cross_validation'"
     ]
    }
   ],
   "source": [
    "from mlfinlab.mlfinlab.microstructural_features.first_generation import get_roll_measure, get_roll_impact,get_corwin_schultz_estimator,get_bekker_parkinson_vol\n",
    "from mlfinlab.mlfinlab.microstructural_features.second_generation import get_bar_based_kyle_lambda, get_bar_based_amihud_lambda, get_bar_based_hasbrouck_lambda\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "from mlfinlab.mlfinlab.features.fracdiff import frac_diff_ffd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getDailyVol(close, span0=100):\n",
    "    # daily vol, reindexed to close\n",
    "    df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))\n",
    "    df0=df0[df0>0]\n",
    "    df0=pd.Series(close.index[df0-1], index=close.index[close.shape[0]-df0.shape[0]:])\n",
    "    df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns\n",
    "    df0=df0.ewm(span=span0).std()\n",
    "    return df0\n",
    "\n",
    "def get_meta_barier(future_window, last_close, min_ret, tp, sl, vertical_zero = True):\n",
    "    '''\n",
    "        From https://github.com/Rachnog/Advanced-Deep-Trading/blob/master/bars-labels-diff/Labeling.ipynb\n",
    "    '''\n",
    "    if vertical_zero:\n",
    "        min_ret_situation = [0, 0, 0]\n",
    "    else:\n",
    "        min_ret_situation = [0, 0]\n",
    "\n",
    "\n",
    "    differences = np.array([(fc - last_close) / last_close for fc in future_window])\n",
    "\n",
    "    # Are there gonna be fluctuations within min_ret???\n",
    "    min_ret_ups = np.where((differences >= min_ret) == True)[0]\n",
    "    min_ret_downs = np.where((differences < -min_ret) == True)[0]\n",
    "\n",
    "    if (len(min_ret_ups) == 0) and (len(min_ret_downs) == 0):\n",
    "        if vertical_zero:\n",
    "            min_ret_situation[2] = 1\n",
    "        else:\n",
    "            if differences[-1] > 0:\n",
    "                min_ret_situation[0] = 1\n",
    "            else:\n",
    "                min_ret_situation[1] = 1\n",
    "    else:\n",
    "        if len(min_ret_ups) == 0: min_ret_ups = [np.inf]\n",
    "        if len(min_ret_downs) == 0: min_ret_downs = [np.inf]\n",
    "\n",
    "        if min_ret_ups[0] < min_ret_downs[0]:\n",
    "            min_ret_situation[0] = 1\n",
    "        else:\n",
    "            min_ret_situation[1] = 1\n",
    "\n",
    "    #  Take profit and stop losses indices\n",
    "    take_profit = np.where((differences >= tp) == True)[0]\n",
    "    stop_loss = np.where((differences < sl) == True)[0]\n",
    "\n",
    "    # Fluctuation directions coincide with take profit / stop loss actions?\n",
    "    if min_ret_situation[0] == 1 and len(take_profit) != 0:\n",
    "        take_action = 1\n",
    "    elif min_ret_situation[1] == 1 and len(stop_loss) != 0:\n",
    "        take_action = 1\n",
    "    else:\n",
    "        take_action = 0.\n",
    "\n",
    "    return min_ret_situation, take_action, [take_profit, stop_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlfinlab.mlfinlab.backtest_statistics.statistics import sharpe_ratio,probabilistic_sharpe_ratio,deflated_sharpe_ratio,information_ratio,minimum_track_record_length,drawdown_and_time_under_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from yahoo_finance import Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Modeling and Training Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    clf=RandomForestClassifier(n_estimators=1,\n",
    "                               criterion='entropy',\n",
    "                               bootstrap=False,\n",
    "                               class_weight='balanced_subsample')\n",
    "    clf=BaggingClassifier(base_estimator=clf,\n",
    "                         n_estimators=100,\n",
    "                         max_features=1.)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_n_times(X_train, Y_train, X_test, Y_test):\n",
    "    mmcs, pred_prs = [], []\n",
    "    for n in range(N_SAMPLES):\n",
    "\n",
    "        clf = create_model()\n",
    "\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        pred_pr = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        mmcs.append(matthews_corrcoef(Y_test, pred))\n",
    "        pred_prs.append(pred_pr)\n",
    "    return mmcs, pred_prs\n",
    "\n",
    "def visualize_mmcs(mmcs):\n",
    "    plt.figure()\n",
    "    plt.title(str(np.mean(mmcs)))\n",
    "    plt.hist(mmcs)\n",
    "    plt.axvline(np.mean(mmcs), color = 'red')\n",
    "    plt.show()\n",
    "    \n",
    "def backtest_predictions(pred_prs, P_test):\n",
    "    bagging_strategies, bagging_returns = [], []\n",
    "    for pred_pr in pred_prs:\n",
    "        signal = [-(1-p) if p <= 0.5 else p for p in pred_pr]\n",
    "        bagging_strategies.append((signal * P_test).cumsum())\n",
    "        bagging_returns.append(signal * P_test)\n",
    "    return bagging_strategies, bagging_returns\n",
    "\n",
    "def visualize_backtests(bagging_strategies, P_test):\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    for strategy in bagging_strategies:\n",
    "        plt.plot(strategy, color = 'grey', ls = '--', lw=0.5)\n",
    "    plt.plot(P_test.cumsum(), lw = 3, label = 'Benchmark')\n",
    "    plt.plot(np.array(bagging_strategies).mean(axis=0), lw = 3, label = 'Strategy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ohclv_to_features(data):\n",
    "    \n",
    "    try:\n",
    "        frac_diff_series = frac_diff_ffd(pd.DataFrame(np.log(data['close'])), 0.7, thresh=1e-4)\n",
    "    except:\n",
    "        print('Not calculated')\n",
    "        frac_diff_series = pd.DataFrame(np.log(data['close'])).pct_change()\n",
    "        \n",
    "    # Add all ta features\n",
    "    technical_features = add_all_ta_features(\n",
    "        data, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\")\n",
    "    \n",
    "    data['feat_tech_volume_cmf'] = technical_features['volume_cmf']\n",
    "    data['feat_tech_volatility_dcl'] = technical_features['volatility_dcl']\n",
    "    data['feat_tech_trend_macd_diff'] = technical_features['trend_macd_diff']\n",
    "    data['feat_tech_trend_vortex_ind_diff'] = technical_features['trend_vortex_ind_diff']\n",
    "    data['feat_tech_momentum_stoch_signal'] = technical_features['momentum_stoch_signal']\n",
    "    data['feat_tech_momentum_tsi'] = technical_features['momentum_tsi']\n",
    "    \n",
    "    data['feat_afml_roll_measure'] = get_roll_measure(data['close'], WINDOW)\n",
    "    data['feat_afml_roll_impact'] = get_roll_impact(data['close'], data['volume'], WINDOW)\n",
    "    data['feat_afml_corwin_schultz'] = get_corwin_schultz_estimator(data['high'], data['low'], WINDOW)\n",
    "    data['feat_afml_bekker_parkinson_vol'] = get_bekker_parkinson_vol(data['high'], data['low'], WINDOW)\n",
    "    data['feat_afml_kyle_lambda'] = get_bar_based_kyle_lambda(data['close'], data['volume'], WINDOW)\n",
    "    data['feat_afml_amihud_lambda'] = get_bar_based_amihud_lambda(data['close'], data['volume'], WINDOW)\n",
    "    data['feat_afml_hasbrouck_lambda'] = get_bar_based_hasbrouck_lambda(data['close'], data['volume'], WINDOW)\n",
    "    \n",
    "    data['feat_stat_min_frac_close'] = frac_diff_series.rolling(WINDOW).min()\n",
    "    data['feat_stat_max_frac_close'] = frac_diff_series.rolling(WINDOW).max()\n",
    "    data['feat_stat_mean_frac_close'] = frac_diff_series.rolling(WINDOW).mean()\n",
    "    data['feat_stat_std_frac_close'] = frac_diff_series.rolling(WINDOW).std()\n",
    "    data['feat_stat_skew_frac_close'] = frac_diff_series.rolling(WINDOW).skew()\n",
    "    data['feat_stat_kurt_frac_close'] = frac_diff_series.rolling(WINDOW).kurt()\n",
    "    data['feat_stat_autocorr_frac_close'] = frac_diff_series.rolling(WINDOW).apply(lambda x: x.autocorr(), raw=False)\n",
    "    \n",
    "    FEATURE_COLUMNS = [d for d in data.columns if 'feat_' in d]\n",
    "    dataset = data[FEATURE_COLUMNS]\n",
    "    \n",
    "    dataset_normalized = {}\n",
    "    for feature_column in dataset.columns:\n",
    "        feature_i = dataset[feature_column]\n",
    "        feature_i_norm = (feature_i - feature_i.rolling(WINDOW).mean()) / feature_i.rolling(WINDOW).std()\n",
    "        dataset_normalized[feature_column] = feature_i_norm\n",
    "    dataset_normalized = pd.DataFrame(\n",
    "           dataset_normalized, index = dataset.index\n",
    "    )\n",
    "    \n",
    "    dataset = dataset_normalized\n",
    "    dataset['close'] = data['close']\n",
    "    dataset = dataset.replace([np.inf, -np.inf], np.nan)\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createXY(inputs, outputs, training=False): \n",
    "    \n",
    "    def flatten_features_window(x):\n",
    "        mean = x.mean(axis=1)\n",
    "        std = x.std(axis=1)\n",
    "        low = x.min(axis=1)\n",
    "        high = x.max(axis=1)\n",
    "        open_f = x[:, 0]\n",
    "        close_f = x[:, -1]\n",
    "        return close_f\n",
    "    \n",
    "    X, Y, P, T = [], [], [], []\n",
    "    \n",
    "    # FIXED IID ASSUMPTION (up to some point)\n",
    "    if training:\n",
    "        SKIP = WINDOW + HORIZON\n",
    "    else:\n",
    "        SKIP = 1\n",
    "    \n",
    "    for i in range(INPUT_WINDOW, len(inputs)-HORIZON, SKIP):\n",
    "\n",
    "        if INPUT_WINDOW > 1:\n",
    "            window = inputs[i-INPUT_WINDOW:i].values\n",
    "        else:\n",
    "            window = inputs.iloc[i].values\n",
    "        future = (outputs[i+HORIZON] - outputs[i]) / outputs[i]\n",
    "        future_binary = 1.0 if future > 0 else 0.0\n",
    "\n",
    "        X.append(window)\n",
    "        Y.append(future_binary)\n",
    "        P.append(future)\n",
    "        T.append(outputs.index[i+HORIZON])\n",
    "\n",
    "    X, Y, P = np.array(X), np.array(Y), np.array(P)\n",
    "    \n",
    "    if INPUT_WINDOW > 1:\n",
    "        X = flatten_features_window(X)\n",
    "    return X, Y, P, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Metrics Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ar1(x):\n",
    "    return np.corrcoef(x[:-1], x[1:])[0,1]\n",
    "\n",
    "def autocorr_penalty(x):\n",
    "    n = len(x)\n",
    "    p = np.abs(ar1(x))\n",
    "    return np.sqrt(1 + 2*np.sum([((n - i)/n)*p**i for i in range(1,n)]))\n",
    "\n",
    "def smart_sharpe(x):\n",
    "    return (np.mean(x)/(np.std(x, ddof=1) * autocorr_penalty(x)) * np.sqrt(252))\n",
    "\n",
    "def calculate_all_metrics(benchmark_returns, strategy_returns, dates_array):\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    benchmark_sharpe = sharpe_ratio(benchmark_returns, entries_per_year=252)\n",
    "    benchmark_sharpe_smart = smart_sharpe(benchmark_returns)\n",
    "    strategy_sharpe = sharpe_ratio(strategy_returns, entries_per_year=252)\n",
    "    strategy_sharpe_smart = smart_sharpe(strategy_returns)\n",
    "    psr = probabilistic_sharpe_ratio(strategy_sharpe, benchmark_sharpe, len(benchmark_returns))\n",
    "    all_strategy_sharpes = []\n",
    "    for r in bagging_returns:\n",
    "        sr_i = sharpe_ratio(r, entries_per_year=252)\n",
    "        all_strategy_sharpes.append(sr_i)\n",
    "    dsr = deflated_sharpe_ratio(strategy_sharpe, all_strategy_sharpes, len(benchmark_returns))\n",
    "    ir = information_ratio(strategy_returns, benchmark=np.mean(benchmark_returns), entries_per_year=252)\n",
    "    mtrl = minimum_track_record_length(strategy_sharpe, benchmark_sharpe)\n",
    "\n",
    "    res['benchmark_mean_returns'] = np.mean(benchmark_returns)\n",
    "    res['strategy_mean_returns'] = np.mean(strategy_returns) \n",
    "    res['benchmark_sharpe'] = benchmark_sharpe\n",
    "    res['benchmark_sharpe_smart'] = benchmark_sharpe_smart\n",
    "    res['strategy_sharpe'] = strategy_sharpe\n",
    "    res['strategy_sharpe_smart'] = strategy_sharpe_smart\n",
    "    res['probabilistic_sharpe_ratio'] = psr\n",
    "    res['deflated_sharpe_ratio'] = dsr\n",
    "    res['information_ratio'] = ir\n",
    "    res['minimum_track_record_length'] = mtrl\n",
    "\n",
    "    return res\n",
    "\n",
    "def calculate_important_features(dataset_train, cutoff_up = 0, cutoff_down = 3, visualize = False):\n",
    "\n",
    "    X_train, Y_train, P_train, T_train = createXY(dataset_train[FEATURE_COLUMNS], dataset_train['close'], training=True)\n",
    "    X_train_df = pd.DataFrame(X_train, columns = FEATURE_COLUMNS)\n",
    "    clf = RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', criterion='entropy')\n",
    "    clf.fit(X_train_df, Y_train)\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    shap_values = explainer.shap_values(X_train_df)\n",
    "    fi0 = np.abs(shap_values[0]).mean(axis=0)\n",
    "    fi1 = np.abs(shap_values[1]).mean(axis=0)\n",
    "    fi = fi0 + fi1\n",
    "    imp = pd.DataFrame({\n",
    "        'feature': X_train_df.columns.tolist(),\n",
    "        'mean': fi\n",
    "    })\n",
    "    imp = imp.set_index('feature')\n",
    "    \n",
    "    if visualize:\n",
    "        imp.sort_values('mean').plot.barh()\n",
    "        \n",
    "    return imp, imp.sort_values('mean')[::-1][cutoff_up:-cutoff_down].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_all_metrics(benchmark_returns, strategy_returns, dates_array, mmcs, pred_prs):\n",
    "    \n",
    "    res = {}\n",
    "    strategy_returns_mean = strategy_returns.mean(axis=0)\n",
    "    \n",
    "    benchmark_sharpe = sharpe_ratio(benchmark_returns, entries_per_year=252)\n",
    "    benchmark_sharpe_smart = smart_sharpe(benchmark_returns)\n",
    "    strategy_sharpe = sharpe_ratio(strategy_returns_mean, entries_per_year=252)\n",
    "    strategy_sharpe_smart = smart_sharpe(strategy_returns_mean)\n",
    "    psr = probabilistic_sharpe_ratio(strategy_sharpe, benchmark_sharpe, len(benchmark_returns))\n",
    "    all_strategy_sharpes = []\n",
    "    for r in strategy_returns:\n",
    "        sr_i = sharpe_ratio(r, entries_per_year=252)\n",
    "        all_strategy_sharpes.append(sr_i)\n",
    "    dsr = deflated_sharpe_ratio(strategy_sharpe, all_strategy_sharpes, len(benchmark_returns))\n",
    "    ir = information_ratio(strategy_returns_mean, benchmark=np.mean(benchmark_returns), entries_per_year=252)\n",
    "    mtrl = minimum_track_record_length(strategy_sharpe, benchmark_sharpe)\n",
    "    \n",
    "    df_for_ddn = pd.DataFrame.from_dict({'Date': dates_array, \n",
    "                                         'Benchmark': benchmark_returns,\n",
    "                                         'Strategy': strategy_returns_mean})\n",
    "    df_for_ddn = df_for_ddn.set_index('Date')\n",
    "    df_for_ddn['Cumulative_Benchmark'] = df_for_ddn.Benchmark.cumsum().round(2)\n",
    "    df_for_ddn['Cumulative_Strategy'] = df_for_ddn.Strategy.cumsum().round(2)\n",
    "    df_for_ddn['HighValue_Benchmark'] = df_for_ddn['Cumulative_Benchmark'].cummax()\n",
    "    df_for_ddn['HighValue_Strategy'] = df_for_ddn['Cumulative_Strategy'].cummax()\n",
    "    df_for_ddn['Drawdown_Benchmark'] = df_for_ddn['Cumulative_Benchmark'] - df_for_ddn['HighValue_Benchmark']\n",
    "    df_for_ddn['Drawdown_Strategy'] = df_for_ddn['Cumulative_Strategy'] - df_for_ddn['HighValue_Strategy']\n",
    "\n",
    "    res['benchmark_mean_returns'] = np.mean(benchmark_returns)\n",
    "    res['strategy_mean_returns'] = np.mean(strategy_returns) \n",
    "    res['benchmark_sharpe'] = benchmark_sharpe\n",
    "    res['benchmark_sharpe_smart'] = benchmark_sharpe_smart\n",
    "    res['strategy_sharpe'] = strategy_sharpe\n",
    "    res['strategy_sharpe_smart'] = strategy_sharpe_smart\n",
    "    res['probabilistic_sharpe_ratio'] = psr\n",
    "    res['deflated_sharpe_ratio'] = dsr\n",
    "    res['information_ratio'] = ir\n",
    "    res['minimum_track_record_length'] = mtrl\n",
    "    res['benchmark_drawdown'] = df_for_ddn['Drawdown_Benchmark'].quantile(.05)\n",
    "    res['strategy_drawdown'] = df_for_ddn['Drawdown_Strategy'].quantile(.05)\n",
    "    res['mmc_mean'] = np.mean(mmcs)\n",
    "    res['mmc_std'] = np.std(mmcs)\n",
    "    res['mmc_sharpe'] = res['mmc_mean'] / res['mmc_std']\n",
    "    res['model_certainty'] = abs(np.array(np.mean(pred_prs)) - 0.5) / 0.5\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TICKER = 'DB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aapl_yf = yahoo_financials_tech = YahooFinancials([TICKER])\n",
    "aapl_yf = Share('YHOO')\n",
    "data = aapl_yf.get_historical_price_data('2000-01-01', '2020-08-01', 'daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data[TICKER]['prices'])\n",
    "data = data.set_index('formatted_date')\n",
    "data_original = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WINDOW = 14\n",
    "INPUT_WINDOW = 1 # REDUCED THE CONTEXT\n",
    "HORIZON = 1\n",
    "N_SAMPLES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = ohclv_to_features(data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = [d for d in dataset.columns if 'feat_' in d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CPCV Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CPCV(BaseCrossValidator):\n",
    "    # TODO: add purge \"holes\" !!!\n",
    "    def __init__(self, X, N, k):\n",
    "        self.X = X\n",
    "        self.N = N\n",
    "        self.k = k\n",
    "\n",
    "    def generate_eras(self):\n",
    "        # assuming exact division, we will cut-off small piece of time series\n",
    "        # in the very beginning\n",
    "        return np.array(sum([\n",
    "                    [i] * (len(self.X) // self.N) for i in range(self.N)\n",
    "                    ], []\n",
    "                   )\n",
    "        )\n",
    "        \n",
    "    def split(self, X=None, y=None, groups=None):\n",
    "        # removing first m items from time series\n",
    "        eras = self.generate_eras()\n",
    "        len_diff = abs(len(self.X) - len(eras))\n",
    "        comb = list(combinations(range(self.N), self.N-self.k))\n",
    "        all_splits = range(self.N)\n",
    "\n",
    "        for combination in comb:\n",
    "            train_indices, test_indices = [], []\n",
    "            for c in combination:\n",
    "                indices_train = list(np.where(eras == c)[0])\n",
    "                train_indices.extend(indices_train)\n",
    "            for t in list(set(all_splits) - set(combination)):\n",
    "                indices_test = list(np.where(eras == t)[0])\n",
    "                test_indices.extend(indices_test)\n",
    "            yield(train_indices, test_indices)  \n",
    "              \n",
    "    def get_n_splits(self):\n",
    "        comb = combinations(range(self.N), self.N-self.k)\n",
    "        return len(list(comb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Probability of strategy overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N = 7\n",
    "k = 3\n",
    "\n",
    "cpcv = CPCV(dataset, N, k)\n",
    "\n",
    "ALL_P_TESTS_OOS = []\n",
    "ALL_RETURNS_OOS = []\n",
    "ALL_DATES_OOS = []\n",
    "ALL_MMCS_OOS, ALL_PREDS_OOS = [], []\n",
    "FEATURE_IMPORTANCES_OOS = []\n",
    "\n",
    "ALL_P_TESTS_IS = []\n",
    "ALL_RETURNS_IS = []\n",
    "ALL_DATES_IS = []\n",
    "ALL_MMCS_IS, ALL_PREDS_IS = [], []\n",
    "FEATURE_IMPORTANCES_IS = []\n",
    "\n",
    "for e, (train_ids, test_ids) in enumerate(cpcv.split()):\n",
    "    print('processing split', e+1, '/', cpcv.get_n_splits())\n",
    "    \n",
    "    dataset_train, dataset_test = dataset.iloc[train_ids], dataset.iloc[test_ids]\n",
    "    imp, IMPORTANT_FEATURES = calculate_important_features(dataset_train)\n",
    "    \n",
    "    X_train, Y_train, P_train, T_train = createXY(dataset_train[FEATURE_COLUMNS], dataset_train['close'], training=True)\n",
    "    X_test, Y_test, P_test, T_test = createXY(dataset_test[FEATURE_COLUMNS], dataset_test['close'])\n",
    "    \n",
    "    mmcs, pred_prs = train_and_evaluate_n_times(X_train, Y_train, X_test, Y_test)\n",
    "    bagging_strategies, bagging_returns = backtest_predictions(pred_prs, P_test)\n",
    "    \n",
    "    ALL_RETURNS_OOS.append(np.array(bagging_returns))\n",
    "    ALL_P_TESTS_OOS.append(P_test)\n",
    "    ALL_DATES_OOS.append(T_test)\n",
    "    ALL_MMCS_OOS.append(mmcs)\n",
    "    ALL_PREDS_OOS.append(pred_prs)\n",
    "    FEATURE_IMPORTANCES_OOS.append(imp)\n",
    "    \n",
    "    mmcs_is, pred_prs_is = train_and_evaluate_n_times(X_train, Y_train, X_train, Y_train)\n",
    "    bagging_strategies_is, bagging_returns_is = backtest_predictions(pred_prs_is, P_train) \n",
    "    \n",
    "    ALL_RETURNS_IS.append(np.array(bagging_returns_is))\n",
    "    ALL_P_TESTS_IS.append(P_train)\n",
    "    ALL_DATES_IS.append(T_train)\n",
    "    ALL_MMCS_IS.append(mmcs_is)\n",
    "    ALL_PREDS_IS.append(pred_prs_is)\n",
    "    FEATURE_IMPORTANCES_IS.append(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results_is = []\n",
    "for b_ret, s_ret, dates, mmcs, preds, fis in zip(ALL_P_TESTS_IS, \n",
    "                                                 ALL_RETURNS_IS, \n",
    "                                                 ALL_DATES_IS, \n",
    "                                                 ALL_MMCS_IS,\n",
    "                                                 ALL_PREDS_IS,\n",
    "                                                 FEATURE_IMPORTANCES_IS):\n",
    "    res = calculate_all_metrics(\n",
    "            np.array(b_ret), \n",
    "            np.array(s_ret), \n",
    "            [datetime.strptime(date, \"%Y-%m-%d\").date() for date in dates],\n",
    "            mmcs, \n",
    "            fis\n",
    "    )\n",
    "    df_results_is.append(res)\n",
    "df_results_is = pd.DataFrame(df_results_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results_oos = []\n",
    "for b_ret, s_ret, dates, mmcs, preds, fis in zip(ALL_P_TESTS_OOS, \n",
    "                                                 ALL_RETURNS_OOS, \n",
    "                                                 ALL_DATES_OOS, \n",
    "                                                 ALL_MMCS_OOS, \n",
    "                                                 ALL_PREDS_OOS, \n",
    "                                                 FEATURE_IMPORTANCES_OOS):\n",
    "    res = calculate_all_metrics(\n",
    "            np.array(b_ret), \n",
    "            np.array(s_ret), \n",
    "            [datetime.strptime(date, \"%Y-%m-%d\").date() for date in dates],\n",
    "            mmcs, \n",
    "            fis\n",
    "    )\n",
    "    df_results_oos.append(res)\n",
    "df_results_oos = pd.DataFrame(df_results_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_c_is = df_results_is['strategy_sharpe']\n",
    "w_c_oos = df_results_oos['strategy_sharpe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(w_c_is.values, label = 'IS Sharpes', alpha = 0.55)\n",
    "plt.hist(w_c_oos.values, label = 'OOS Sharpes', alpha = 0.55)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = w_c_oos\n",
    "X = w_c_is\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(np.min(w_c_is), np.max(w_c_is), 0.5)\n",
    "y = 0.3459 + -0.0236 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(w_c_is, w_c_oos, label = 'Backtests IS vs OOS Sharpes')\n",
    "plt.plot(x, y, color = 'red', ls = '--', label = 'Regression between IS and OOS Sharpes')\n",
    "plt.xlabel('Sharpe ratios IS')\n",
    "plt.ylabel('Sharpe ratios OOS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_star = np.argmax(w_c_is)\n",
    "w_c = (ss.rankdata(w_c_oos) - n_star) / len(w_c_oos)\n",
    "w_c = w_c + np.abs(np.min(w_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_c = np.log(w_c / (1 - w_c))\n",
    "y_c[y_c==-np.inf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_c_neg = y_c[y_c < 0]\n",
    "y_c_neg = (y_c_neg - y_c_neg.min()) / (y_c_neg.max() - y_c_neg.min())\n",
    "pbo = np.mean(y_c_neg)\n",
    "pbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_c, label = 'Logits $w_c$, PBO = ' + str(np.round(pbo, 2)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Strategy risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def binHR(sl, pt, n, tSR):\n",
    "    a = (n + tSR**2) * (pt - sl)**2\n",
    "    b = (2*n*sl - tSR**2*(pt-sl))*(pt-sl)\n",
    "    c = n*sl**2\n",
    "    p = (-b + (b**2 - 4*a*c)**0.5) / (2.*a)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def probFailure(ret, freq, tSR):\n",
    "    # Derive probability that strategy may fail\n",
    "    rPos, rNeg = ret[ret>0].mean(), ret[ret<=0].mean()\n",
    "    p = ret[ret>0].shape[0] / float(ret.shape[0])\n",
    "    thresP = binHR(rNeg, rPos, freq, tSR)\n",
    "    risk = ss.norm.cdf(thresP, p, p*(1-p)) # approximation to bootstrap\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "failure_probs = []\n",
    "for returns in ALL_RETURNS_OOS:\n",
    "    some_returns = returns.mean(axis=0)\n",
    "    pf = probFailure(some_returns, 260, 0.0)\n",
    "    failure_probs.append(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(failure_probs, label = 'Failure prob for Sharpe 0.0')\n",
    "plt.axvline(0.5, ls = '--', color = 'grey', label = '50% probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(ALL_RETURNS_OOS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}